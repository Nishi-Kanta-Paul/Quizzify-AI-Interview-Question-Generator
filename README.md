# ğŸ§  Quizzify: AI Interview Question Generator

**Quizzify** is a privacy-first, fully offline tool that generates realistic technical interview questions and answers using **local AI models**. Whether you're preparing for a **Software Engineer** or **Data Scientist** role, Quizzify tailors questions to your job role and difficulty levelâ€”all without sending your data to the cloud.

---

## ğŸŒŸ Features

- âœ… **100% Offline** â€“ All data and AI inference run locally on your machine.
- ğŸ‘©â€ğŸ’» **Role-based Q&A** â€“ Get questions for any job role (e.g., Backend Developer, AI Engineer).
- ğŸšï¸ **Difficulty Levels** â€“ Choose between **Beginner** and **Advanced** modes.
- ğŸ“ **Export to Markdown** â€“ Save your generated questions as a `.md` file.
- ğŸ¤– **LLM Powered** â€“ Uses [Ollama](https://ollama.com) to run large language models locally.
- ğŸ’» **Modern Web UI** â€“ Clean and responsive Flask-based interface.

---

## ğŸ§ How It Works

1. **Prompt Loading** â€“ Custom prompt templates are used based on the selected job role and difficulty.
2. **LLM Generation** â€“ Sends prompts to your local LLM using Ollama.
3. **Display & Export** â€“ Questions and answers appear in the web UI with an option to export.

---

## ğŸ› ï¸ Requirements

- Python 3.8 or higher
- [Ollama](https://ollama.com) installed and added to your PATH
- Dependencies listed in `requirements.txt`

---

## âš¡ Quickstart

### 1. Clone the Repository

```bash
git clone https://github.com/Nishi-Kanta-Paul/Quizzify-AI-Interview-Question-Generator.git
cd ai_interview_generator
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Install & Configure Ollama

- Download Ollama: [https://ollama.com](https://ollama.com)
- Add `ollama` to your system PATH
- Test it:

```bash
ollama --version
```

- Pull a local LLM (e.g., `phi`):

```bash
ollama pull phi
```

### 4. Run the App

```bash
python app.py
```

Open in your browser:

ğŸ‘‰ [http://localhost:5000](http://localhost:5000)

---

## ğŸ–¥ï¸ Usage

1. Navigate to [http://localhost:5000](http://localhost:5000)
2. Input a job role (e.g., "Frontend Developer")
3. Select number of questions and difficulty
4. Click **Generate Questions**
5. Optionally, click **Download Markdown** to save your results

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                 # Flask web app
â”œâ”€â”€ generator.py           # Main logic for generating Q&A
â”œâ”€â”€ ollama_interface.py    # Handles local LLM interactions
â”œâ”€â”€ utils.py               # Helper functions
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ base_prompt.txt    # Prompt template for LLM
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css          # Styling for the UI
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Web UI template
â””â”€â”€ __pycache__/           # Python cache (auto-generated)
```

---

## â“ FAQ

**Q: Is my data sent to the cloud?**

**A:** Never. All processing happens locally via Ollama.

**Q: Can I use a different LLM model?**

**A:** Yes. Modify the model name in `ollama_interface.py` (default is `phi`).

**Q: Why is the first run slow?**

**A:** The model is downloaded and loaded into memory. Future runs are much faster.

**Q: Can I use this for non-technical roles?**

**A:** Absolutely. Just input any job title you want!

---

## ğŸ›  Troubleshooting

**Problem:** `ollama` not found

**Solution:**

- Ensure Ollama is installed and added to PATH
- Restart terminal after installation

**Problem:** Timeout or no response

**Solution:**

- Wait for the model to fully load
- Increase timeout in `ollama_interface.py` if necessary
- Check terminal for error messages

---

## ğŸ¤ Contributing

We welcome contributions! Please open an issue or submit a pull request with improvements or feature suggestions.

---

## ğŸ“œ License

This project is licensed under the **MIT License**. See `LICENSE` for details.

---

## ğŸ™ Credits

- [Ollama](https://ollama.com) â€“ For enabling local LLM inference
- [Flask](https://flask.palletsprojects.com) â€“ For building the web UI
- Built with â¤ï¸ for **privacy-conscious** interview preparation
