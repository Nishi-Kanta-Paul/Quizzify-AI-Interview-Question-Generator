ğŸ§  Quizzify: AI Interview Question Generator

Quizzify is a privacy-first, offline tool that generates realistic technical interview questions and answers for any job role using local AI models. Choose your role, set the difficulty, and get instant, context-aware questionsâ€”no internet or cloud required.

ğŸŒŸ Features

- 100% Offline: All processing is localâ€”your data never leaves your computer.
- Role-based Q&A: Generate questions for any job role (e.g., Data Scientist, Software Engineer).
- Difficulty Modes: Choose Beginner or Advanced for tailored questions and answers.
- Save to Markdown: Download your generated questions as a Markdown file.
- LLM Powered: Uses Ollama to run large language models locally.
- Modern Web UI: Clean, responsive interface for easy interaction.

ğŸ§ How It Works

1. Prompt Template: Loads a customizable prompt for your selected job role and settings.
2. LLM Generation: Sends the prompt to a local LLM (via Ollama) for question and answer generation.
3. Output: Displays the results in the web UI and allows you to save them as a Markdown file.

ğŸ› ï¸ Requirements

- Python 3.8 or higher
- Ollama (for local LLM inference)
- See requirements.txt for Python dependencies

âš¡ Quickstart

Clone the repository:

```sh
git clone https://github.com/Nishi-Kanta-Paul/Quizzify-AI-Interview-Question-Generator.git
cd ai_interview_generator
```

Install Python dependencies:

```sh
pip install -r requirements.txt
```

Install Ollama and add to PATH:

- Download from [Ollama.com](https://ollama.com/)
- Make sure `ollama` runs from your terminal (test with `ollama --version`)
- Pull a model (e.g., `phi`):
  ```sh
  ollama pull phi
  ```

Run the app:

```sh
python app.py
```

Open your browser: Go to http://localhost:5000

ï¿½ï¸ Usage

Web App

- Go to http://localhost:5000
- Enter a job role, number of questions, and select mode
- Click "Generate Questions"
- Optionally, save the questions to a Markdown file

ğŸ§© Project Structure

â”œâ”€â”€ app.py # Flask web app
â”œâ”€â”€ generator.py # Generates questions using Ollama
â”œâ”€â”€ ollama_interface.py # LLM interaction
â”œâ”€â”€ utils.py # Utility functions
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ prompts/
â”‚ â””â”€â”€ base_prompt.txt # Prompt template
â”œâ”€â”€ static/
â”‚ â””â”€â”€ style.css # App styling
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html # Web UI
â””â”€â”€ **pycache**/ # Python cache (gitignored)

ï¿½ Troubleshooting

Ollama not found?

- Make sure Ollama is installed and its directory is in your system PATH.
- Restart your terminal after installation.

No response or timeout?

- The first run may take longer as the model is downloaded and loaded.
- Increase the timeout in `ollama_interface.py` if needed.
- Check for errors in the terminal where you started the Flask app.

â“ FAQ

**Q: Is my data sent to the cloud?**  
A: No. All processing is local. Your job roles and questions never leave your machine.

**Q: Can I use my own LLM model?**  
A: Yes! You can change the model name in `ollama_interface.py` (default is `phi`).

**Q: Why is it slow the first time?**  
A: The model may need to be downloaded and loaded into memory. Subsequent runs are faster.

**Q: Can I use this for non-technical roles?**  
A: Yes! Just enter any job role you want.

ğŸ¤ Contributing

Pull requests and suggestions are welcome! Please open an issue or submit a PR.

ğŸ“œ License

This project is licensed under the MIT License.

ğŸ™ Credits

Ollama for local LLM serving  
Flask  
Made with â¤ï¸ for privacy-conscious interview preparation.
